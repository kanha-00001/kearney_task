{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da01d9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1021 20:39:44.112000 19280 site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hashlib\n",
    "import tempfile\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, Document\n",
    "from llama_index.core.node_parser import SentenceWindowNodeParser\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor, SentenceTransformerRerank\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.groq import Groq\n",
    "from llama_index.core.settings import Settings\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f408944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_llm():\n",
    "    \"\"\"Initialize and return the Groq LLM.\"\"\"\n",
    "    load_dotenv()\n",
    "    api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"GROQ_API_KEY not found in .env file\")\n",
    "    os.environ[\"GROQ_API_KEY\"] = api_key\n",
    "    return Groq(model=\"llama3-70b-8192\", temperature=0.1)\n",
    "\n",
    "def load_document(file_content, file_name):\n",
    "    \"\"\"\n",
    "    Load a document from file content and return a Document object.\n",
    "    Modified to handle CSV specifically by converting to markdown table for better RAG.\n",
    "    \n",
    "    Args:\n",
    "        file_content (bytes): Content of the uploaded file.\n",
    "        file_name (str): Name of the uploaded file.\n",
    "        \n",
    "    Returns:\n",
    "        Document: LlamaIndex Document object.\n",
    "    \"\"\"\n",
    "    if not file_name.lower().endswith('.csv'):\n",
    "        raise ValueError(\"Only CSV files are supported for this RAG tool.\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(file_content)\n",
    "        markdown_table = df.to_markdown(index=False)\n",
    "        return Document(text=markdown_table)\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing CSV: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3472fa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_sentence_window_index(documents, collection_name, sentence_window_size=3):\n",
    "    \"\"\"\n",
    "    Build a sentence window index using Chroma vector store.\n",
    "    \n",
    "    Args:\n",
    "        documents (list): List of Document objects.\n",
    "        collection_name (str): Name of the Chroma collection.\n",
    "        sentence_window_size (int): Size of the sentence window.\n",
    "        \n",
    "    Returns:\n",
    "        VectorStoreIndex: LlamaIndex vector store index.\n",
    "    \"\"\"\n",
    "    node_parser = SentenceWindowNodeParser.from_defaults(\n",
    "        window_size=sentence_window_size,\n",
    "        window_metadata_key=\"window\",\n",
    "        original_text_metadata_key=\"original_text\",\n",
    "    )\n",
    "    embedding_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "    llm = initialize_llm()\n",
    "    Settings.llm = llm\n",
    "    Settings.embed_model = embedding_model\n",
    "    Settings.node_parser = node_parser\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    chroma_collection = chroma_client.get_or_create_collection(name=collection_name)\n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    index = VectorStoreIndex.from_documents(\n",
    "        documents,\n",
    "        vector_store=vector_store\n",
    "    )\n",
    "    return index\n",
    "\n",
    "def get_sentence_window_query_engine(index, similarity_top_k=6, rerank_top_n=2):\n",
    "    \"\"\"\n",
    "    Create a query engine for the sentence window index.\n",
    "    \n",
    "    Args:\n",
    "        index (VectorStoreIndex): LlamaIndex vector store index.\n",
    "        similarity_top_k (int): Number of top similar nodes to retrieve.\n",
    "        rerank_top_n (int): Number of nodes to rerank.\n",
    "        \n",
    "    Returns:\n",
    "        QueryEngine: LlamaIndex query engine.\n",
    "    \"\"\"\n",
    "    postproc = MetadataReplacementPostProcessor(target_metadata_key=\"window\")\n",
    "    rerank = SentenceTransformerRerank(\n",
    "        top_n=rerank_top_n,\n",
    "        model=\"BAAI/bge-reranker-base\"\n",
    "    )\n",
    "    return index.as_query_engine(\n",
    "        similarity_top_k=similarity_top_k,\n",
    "        node_postprocessors=[postproc, rerank]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45dcf979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from rag_tool import process_file_and_get_query_engine\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "query_engine = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e839ae2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
